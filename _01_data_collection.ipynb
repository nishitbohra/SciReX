{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CezC3Jaz_VIl",
        "outputId": "70a8f899-d40f-4178-e48e-9f2f0ebec82a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (6.0.11)\n",
            "Requirement already satisfied: nltk in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: click in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\kanchan\\desktop\\scirex\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Kanchan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Kanchan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 200 papers with query: cat:cs.AI+AND+all:generative+ai\n",
            "Fetched 100 papers so far...\n",
            "Fetched 200 papers so far...\n",
            "Total papers fetched: 200\n",
            "Saved processed data to data/cleaned/generative_ai_papers.csv\n",
            "Saved processed data to data/cleaned/generative_ai_papers.csv\n",
            "\n",
            "Dataset Statistics:\n",
            "Number of papers: 200\n",
            "Average title length: 11.16 tokens\n",
            "Average abstract length: 192.43 tokens\n",
            "Papers with potential datasets mentioned: 200\n"
          ]
        }
      ],
      "source": [
        "%pip install feedparser nltk \n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datetime import datetime\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to fetch papers from arXiv API\n",
        "def fetch_arxiv_papers(query='cat:cs.AI+AND+all:generative+ai', max_results=100):\n",
        "    \"\"\"\n",
        "    Fetch papers from arXiv API based on query\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    query : str\n",
        "        arXiv query string (e.g., 'cat:cs.AI+AND+all:generative+ai')\n",
        "    max_results : int\n",
        "        Maximum number of results to return\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame containing paper details\n",
        "    \"\"\"\n",
        "    print(f\"Fetching {max_results} papers with query: {query}\")\n",
        "\n",
        "    # Initialize empty list to store paper details\n",
        "    papers = []\n",
        "\n",
        "    # Set up arXiv API URL with query parameters\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    start = 0\n",
        "    batch_size = 100  # arXiv allows max 100 results per request\n",
        "\n",
        "    # Loop to fetch papers in batches\n",
        "    while start < max_results:\n",
        "        # Calculate remaining papers to fetch\n",
        "        current_batch_size = min(batch_size, max_results - start)\n",
        "\n",
        "        # Construct the query URL\n",
        "        query_url = f\"{base_url}search_query={query}&start={start}&max_results={current_batch_size}&sortBy=submittedDate&sortOrder=descending\"\n",
        "\n",
        "        # Fetch data from arXiv\n",
        "        response = feedparser.parse(query_url)\n",
        "\n",
        "        # Process each entry\n",
        "        for entry in response.entries:\n",
        "            # Extract authors\n",
        "            authors = [author.name for author in entry.authors]\n",
        "\n",
        "            # Extract categories\n",
        "            categories = [tag['term'] for tag in entry.tags]\n",
        "\n",
        "            # Create paper dictionary\n",
        "            paper = {\n",
        "                'id': entry.id.split('/abs/')[-1],\n",
        "                'title': entry.title.replace('\\n', ' ').strip(),\n",
        "                'abstract': entry.summary.replace('\\n', ' ').strip(),\n",
        "                'authors': ', '.join(authors),\n",
        "                'categories': ', '.join(categories),\n",
        "                'published': entry.published,\n",
        "                'link': entry.link\n",
        "            }\n",
        "\n",
        "            papers.append(paper)\n",
        "\n",
        "        # Update counter for next batch\n",
        "        start += current_batch_size\n",
        "\n",
        "        # Sleep to prevent hitting API rate limits\n",
        "        print(f\"Fetched {len(papers)} papers so far...\")\n",
        "        time.sleep(3)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(papers)\n",
        "    print(f\"Total papers fetched: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing special characters, extra spaces, etc.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove LaTeX commands\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Function to extract datasets mentioned in the text\n",
        "def extract_datasets(text):\n",
        "    \"\"\"\n",
        "    Extract potential dataset names from text\n",
        "    This is a simple rule-based approach, can be improved with NER models\n",
        "    \"\"\"\n",
        "    # Common dataset name patterns\n",
        "    dataset_patterns = [\n",
        "        r'\\b[A-Z0-9]+-?[A-Za-z0-9]+(?:-[0-9]+)?\\b',  # MNIST, MS-COCO, ImageNet-1K\n",
        "        r'\\b[A-Za-z]+(?:Dataset|Corpus|DB)\\b',       # PennTreebank, SQuADDataset\n",
        "        r'\\b(?:dataset|corpus|database)s?\\s+(?:called|named)\\s+([A-Za-z0-9]+)',  # dataset called XYZ\n",
        "    ]\n",
        "\n",
        "    potential_datasets = []\n",
        "    for pattern in dataset_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        potential_datasets.extend([m for m in matches if len(m) > 2])\n",
        "\n",
        "    # Filter common false positives\n",
        "    stopwords = ['THE', 'AND', 'FOR', 'WITH', 'THIS', 'THAT', 'USING', 'WE', 'THEY']\n",
        "    filtered_datasets = [d for d in potential_datasets if d.upper() not in stopwords]\n",
        "\n",
        "    return list(set(filtered_datasets))\n",
        "\n",
        "# Fetch papers\n",
        "papers_df = fetch_arxiv_papers(query='cat:cs.AI+AND+all:generative+ai', max_results=200)\n",
        "\n",
        "# Clean titles and abstracts\n",
        "papers_df['cleaned_title'] = papers_df['title'].apply(clean_text)\n",
        "papers_df['cleaned_abstract'] = papers_df['abstract'].apply(clean_text)\n",
        "\n",
        "# Extract tokens\n",
        "papers_df['title_tokens'] = papers_df['cleaned_title'].apply(word_tokenize)\n",
        "papers_df['abstract_tokens'] = papers_df['cleaned_abstract'].apply(word_tokenize)\n",
        "\n",
        "# Count tokens\n",
        "papers_df['title_token_count'] = papers_df['title_tokens'].apply(len)\n",
        "papers_df['abstract_token_count'] = papers_df['abstract_tokens'].apply(len)\n",
        "\n",
        "# Extract potential datasets\n",
        "papers_df['potential_datasets'] = papers_df['abstract'].apply(extract_datasets)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = 'data/cleaned/generative_ai_papers.csv'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "papers_df.to_csv(output_file, index=False)\n",
        "print(f\"Saved processed data to {output_file}\")\n",
        "papers_df.to_csv(output_file, index=False)\n",
        "print(f\"Saved processed data to {output_file}\")\n",
        "\n",
        "# Display statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Number of papers: {len(papers_df)}\")\n",
        "print(f\"Average title length: {papers_df['title_token_count'].mean():.2f} tokens\")\n",
        "print(f\"Average abstract length: {papers_df['abstract_token_count'].mean():.2f} tokens\")\n",
        "print(f\"Papers with potential datasets mentioned: {(papers_df['potential_datasets'].str.len() > 0).sum()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
